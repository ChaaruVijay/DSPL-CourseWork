{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ChaaruVijay/DSPL-CourseWork/blob/main/Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "EsfXWZK3DK3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize"
      ],
      "metadata": {
        "id": "TKVS_J48DNRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "ccAb2TuzAFR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
      ],
      "metadata": {
        "id": "2sXZKGUADPKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('cleaned_train.csv')"
      ],
      "metadata": {
        "id": "lIT0jyu7DXBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select features and target variable\n",
        "X = df[['luxury_sales', 'fresh_sales', 'dry_sales', 'outlet_city']]\n",
        "y = df['cluster_category']"
      ],
      "metadata": {
        "id": "dISNSpzaDZ5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)"
      ],
      "metadata": {
        "id": "2GncH7EnDbsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numerical features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "--e7aj3ND1nV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training & testing sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3QXKMbwwESDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Standardize before PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=0.95)  # Retain 95% variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(f\"Original features: {X_train.shape[1]}, Reduced features: {X_pca.shape[1]}\")"
      ],
      "metadata": {
        "id": "J5C82VKokUIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Optimized Neural Network Model\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.001), input_shape=(X_train.shape[1],)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(128, activation='relu', kernel_regularizer=l2(0.001)),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "DOI5Oi0lEiwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "optimizer = Adam(learning_rate=0.0003)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DXigrlaqHN0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Gaussian noise to training data for better generalization\n",
        "noise_factor = 0.005  # Slightly reduced noise\n",
        "X_train_noisy = X_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X_train.shape)"
      ],
      "metadata": {
        "id": "LCvHBgd_EWI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks: Early Stopping & Learning Rate Reduction\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1)"
      ],
      "metadata": {
        "id": "jjKotY81HQwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from sklearn.utils.class_weight import compute_class_weight\n",
        "#import numpy as np\n",
        "\n",
        "#class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "#class_weight_dict = dict(enumerate(class_weights))"
      ],
      "metadata": {
        "id": "BeL1pmVlgheG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "\n",
        "# Compute class weights dynamically\n",
        "computed_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = dict(enumerate(computed_weights, start=1))  # Ensure class indices start from 1\n",
        "\n",
        "# Override only class 5 weight while keeping others as computed\n",
        "class_weight_dict[5] = 2.0  # Manually adjusting class 5\n",
        "\n",
        "print(\"Final Class Weights:\", class_weight_dict)"
      ],
      "metadata": {
        "id": "xvJrSK3N1aWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE"
      ],
      "metadata": {
        "id": "REynIqo5m1Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply SMOTE to the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Train the model on the SMOTE-balanced data\n",
        "# (Make sure you have X_train_smote and y_train_smote defined)\n",
        "model.fit(X_train_smote, y_train_smote, epochs=50, batch_size=64, validation_split=0.2,\n",
        "          callbacks=[early_stopping, lr_scheduler], class_weight=class_weight_dict)"
      ],
      "metadata": {
        "id": "rzl3UcW0mzMq",
        "outputId": "35ad7e90-a69d-4d31-b984-b3ca734da7ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m11318/11318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 6ms/step - accuracy: 0.8879 - loss: 0.5461 - val_accuracy: 0.3411 - val_loss: 0.9223 - learning_rate: 3.0000e-04\n",
            "Epoch 2/50\n",
            "\u001b[1m11318/11318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6ms/step - accuracy: 0.9493 - loss: 0.1195 - val_accuracy: 0.7152 - val_loss: 0.7839 - learning_rate: 3.0000e-04\n",
            "Epoch 3/50\n",
            "\u001b[1m11318/11318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 6ms/step - accuracy: 0.9544 - loss: 0.1033 - val_accuracy: 0.5538 - val_loss: 0.7913 - learning_rate: 3.0000e-04\n",
            "Epoch 4/50\n",
            "\u001b[1m11318/11318\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 6ms/step - accuracy: 0.9548 - loss: 0.1007 - val_accuracy: 0.5171 - val_loss: 0.7528 - learning_rate: 3.0000e-04\n",
            "Epoch 5/50\n",
            "\u001b[1m 8475/11318\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.9555 - loss: 0.0985"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "#history = model.fit(X_train_noisy, y_train, epochs=50, batch_size=256, validation_data=(X_test, y_test), callbacks=[early_stopping, lr_scheduler]) # Use class_weights_dict instead of class_weights"
      ],
      "metadata": {
        "id": "aIa_tUELOMqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_train, y_train)\n",
        "print(f\"Train Loss: {loss:.4f}\")\n",
        "print(f\"Train Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "IaQSe2rjYdi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "n9owAvvcHVuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot Training & Validation Loss\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['loss'], label='Training Loss', color='blue')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training vs Validation Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot Training & Validation Accuracy\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy', color='blue')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='red')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d7YX_K8pHvb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict probabilities\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = np.argmax(y_pred_prob, axis=1)"
      ],
      "metadata": {
        "id": "rQRETn4-HyF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "67xYMMU8H0zq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification Report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=[str(c) for c in label_encoder.classes_]))"
      ],
      "metadata": {
        "id": "OGo2eRJyH4Ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "# SHAP Explainer\n",
        "explainer = shap.Explainer(model, X_train)\n",
        "shap_values = explainer(X_test[:100])\n",
        "\n",
        "# Summary plot\n",
        "shap.summary_plot(shap_values, X_test[:100])"
      ],
      "metadata": {
        "id": "xY1y3Lo-jMOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load cleaned test dataset\n",
        "cleaned_test = pd.read_csv('cleaned_test.csv')"
      ],
      "metadata": {
        "id": "3yuTuQDvIEQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the same features used in training\n",
        "X_cleaned_test = cleaned_test[['luxury_sales', 'fresh_sales', 'dry_sales', 'outlet_city']]"
      ],
      "metadata": {
        "id": "2RM5lJYpIS5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict cluster categories\n",
        "cleaned_test_predictions = model.predict(X_cleaned_test)\n",
        "cleaned_test_predictions = np.argmax(cleaned_test_predictions, axis=1)"
      ],
      "metadata": {
        "id": "UYW4FQxZIZhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save predictions\n",
        "cleaned_test.to_csv('cleaned_test_predictions.csv', index=False)\n",
        "\n",
        "print(\"Predictions saved successfully!\")"
      ],
      "metadata": {
        "id": "vJCAfjaQIcp7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}